{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Load_local_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPE9K+VA2dFw146lpUPLYHC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"duYhPVXL9ZMo"},"outputs":[],"source":["# Load EMNIST image\n","def Load_EMNIST(test_index, algorithm):\n","    # define information about EMNIST dataset\n","    number = 128\n","    classes = 8\n","    character_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n","\n","    # load the whole dataset\n","    x_total = []\n","    y_total = []\n","    for i, c in enumerate(character_list):\n","        for j in range(number):\n","            #x_total.append(cv2.imread('./EMNIST/'+c+'/'+str(j)+'.jpg'))\n","            x_total.append(cv2.imread('/content/drive/MyDrive/Colab Notebooks/AI/EMNIST/' + c + '/'+str(j) + '.jpg'))\n","            y_total.append(i)\n","    \n","    # seperate dataset into training and test set\n","    # initialize as any array\n","    x_train, y_train, x_test, y_test = [], [], [], []\n","    \n","    # if we're not applying CNN, then flatten the image into 1D array\n","    if algorithm == 0:\n","        x_flatten = []\n","        for i in range(len(x_total)):\n","            x_flatten.append(x_total[i].flatten())\n","        x_train, y_train, x_test, y_test = Seperate_Dataset(x_flatten, y_total, test_index, number, classes)\n","    # CNN use 2D image\n","    elif algorithm == 1: \n","        x_train, y_train, x_test, y_test = Seperate_Dataset(x_total, y_total, test_index, number, classes)\n","    \n","    # convert list to ndarray\n","    x_train = np.array(x_train)\n","    y_train = np.array(y_train)\n","    x_test = np.array(x_test)\n","    y_test = np.array(y_test)\n","\n","    # normalize image by dividing 255\n","    x_train = x_train / 255.0\n","    x_test = x_test / 255.0\n","    \n","    # one-hot encoding for label\n","    y_train = One_hot_encoding(y_train, classes)\n","    y_test = One_hot_encoding(y_test, classes)\n","    \n","    return x_train, y_train, x_test, y_test"]},{"cell_type":"code","source":["# seperate dataset into training set and test set\n","def Seperate_Dataset(x_total, y_total, test_index, number, classes):\n","    x_train = []\n","    y_train_temp = []\n","    x_test = []\n","    y_test_temp = []\n","    \n","    # seperate dataset\n","    for i in range(classes):\n","        character_type = 128 * i\n","        for j in range(number):\n","            index = character_type + j\n","            if j in test_index:\n","                x_test.append(x_total[index])\n","                y_test_temp.append(y_total[index])\n","            else:\n","                x_train.append(x_total[index])\n","                y_train_temp.append(y_total[index])\n","    \n","    return x_train, y_train_temp, x_test, y_test_temp"],"metadata":{"id":"6VrNJpvb91wO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# one-hot encoding for label\n","def One_hot_encoding(x, classes):\n","    one_hot = np.zeros((x.shape[0], classes))\n","    for i in range(x.shape[0]):\n","        one_hot[i][x[i]] = 1\n","    \n","    return one_hot"],"metadata":{"id":"-MoTnP6Q950s"},"execution_count":null,"outputs":[]}]}