{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic_Regression.ipynb","provenance":[],"authorship_tag":"ABX9TyNuKvQkzORhvLrmYwKVUNYI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"57qWFBjX-PT-"},"outputs":[],"source":["# for logistic regression\n","class Logistic_Regression():\n","    def __init__(self, x_train, y_train, x_test, y_test, classes, learning_rate):\n","        self.x_train, self.y_train, self.x_test, self.y_test = x_train, y_train, x_test, y_test\n","        self.classes = classes\n","        self.eta = learning_rate\n","        \n","        # initial weight to be 0 everywhere\n","        #self.W = np.random.uniform(size=(classes, x_train.shape[1]))\n","        self.W = np.zeros((self.classes, x_train.shape[1]))\n","        \n","        # learning history\n","        self.train_loss, self.train_precision = [], []\n","        self.test_loss, self.test_precision = [], []\n","    \n","    # calculate the prediction\n","    def Prediction(self):\n","        self.x_train_pred = np.dot(self.x_train, self.W.T)\n","        self.x_test_pred = np.dot(self.x_test, self.W.T)\n","    \n","    # use softmax to normalize the probability\n","    def Softmax(self):\n","        # training set\n","        exp_z = np.exp(self.x_train_pred)\n","        exp_sum = np.sum(exp_z, axis=1).reshape(exp_z.shape[0], 1)\n","        self.train_pred = exp_z / exp_sum\n","        \n","        # test set\n","        exp_z = np.exp(self.x_test_pred)\n","        exp_sum = np.sum(exp_z, axis=1).reshape(exp_z.shape[0], 1)\n","        self.test_pred = exp_z / exp_sum\n","    \n","    # esitmation prediction perfomance\n","    def Cross_entropy(self):\n","        # training set\n","        n = self.y_train.shape[0]\n","        error = 0\n","        q = np.log2(self.train_pred)\n","        for i in range(n):\n","            for j in range(self.classes):\n","                error -= self.y_train[i][j]*q[i][j]\n","        self.train_loss.append(error / n)\n","        \n","        # test set\n","        n = self.y_test.shape[0]\n","        error = 0\n","        q = np.log2(self.test_pred)\n","        for i in range(n):\n","            for j in range(self.classes):\n","                error -= self.y_test[i][j]*q[i][j]\n","        self.test_loss.append(error / n)\n","    \n","    # calculate precision\n","    def Precision(self):\n","        # training set\n","        one_hot_pred = np.argmax(self.train_pred, axis=1)\n","        one_hot_y = np.argmax(self.y_train, axis=1)\n","        truth = one_hot_pred.shape[0]\n","        count = truth\n","        for i in range(truth):\n","            if one_hot_pred[i] != one_hot_y[i]:\n","                count -= 1\n","        self.train_precision.append(count / truth)\n","        \n","        # test set\n","        one_hot_pred = np.argmax(self.test_pred, axis=1)\n","        one_hot_y = np.argmax(self.y_test, axis=1)\n","        truth = one_hot_pred.shape[0]\n","        count = truth\n","        for i in range(truth):\n","            if one_hot_pred[i] != one_hot_y[i]:\n","                count -= 1\n","        self.test_precision.append(count / truth)\n","    \n","    # back-propagation of (softmax + cross-entropy)\n","    def Derivative_cross_entropy(self, y_pred, y):\n","        self.derivative = y_pred - y\n","    \n","    # batch gradient descent\n","    def Batch_GD(self):\n","        self.Derivative_cross_entropy(self.train_pred, self.y_train)\n","        gradient = np.dot(self.derivative.T, self.x_train)\n","        self.W -= gradient * self.eta\n","    \n","    # stochastic gradient descent, with 32 iteration for each epoch\n","    def SGD(self):\n","        for i in range(32):\n","            id = np.random.randint(self.y_train.shape[0], size=1)\n","            self.Derivative_cross_entropy(self.train_pred[id], self.y_train[id])\n","            gradient = np.dot(self.derivative.T, self.x_train[id])\n","            self.W -= gradient * self.eta\n","    \n","    # minibatch gradient descent, with user-defined minibatch\n","    def Minibatch_SGD(self):\n","        id = np.random.choice(self.y_train.shape[0], self.minibatch, replace=False)\n","\n","        sgd_x = np.zeros((self.minibatch, self.x_train.shape[1]))\n","        sgd_y = np.zeros((self.minibatch, self.classes))\n","        sgd_y_pred = np.zeros((self.minibatch, self.classes))\n","        for i in range(self.minibatch):\n","            sgd_x[i, :] = self.x_train[id[i], :]\n","            sgd_y[i, :] = self.y_train[id[i], :]\n","            sgd_y_pred[i, :] = self.train_pred[id[i], :]\n","        \n","        self.Derivative_cross_entropy(sgd_y_pred, sgd_y)\n","        gradient = np.dot(self.derivative.T, sgd_x)\n","        self.W -= gradient * self.eta\n","    \n","    # plot learning curve\n","    def Plot(self):\n","        x_axis = list(range(self.epoch))\n","        # plot loss curve\n","        plt.subplot(1, 2, 1)\n","        plt.plot(x_axis, self.train_loss, color='blue', label='training loss')\n","        plt.plot(x_axis, self.test_loss, color='orange', label='testing loss')\n","        plt.legend()\n","        \n","        # plot precision curve\n","        plt.subplot(1, 2, 2)\n","        percetage_train = [round(i, 2)*100 for i in self.train_precision]\n","        percetage_test = [round(i, 2)*100 for i in self.test_precision]\n","        plt.plot(x_axis, percetage_train, color='blue', label='training accuracy')\n","        plt.plot(x_axis, percetage_test, color='orange', label='testing accuracy')\n","        plt.legend()\n","\n","        plt.show()\n","    \n","    def Display(self):\n","        print('for training data, the final classification accuracy = '+str(round(self.train_precision[self.epoch-1], 5))+\n","              ', and loss = '+str(round(self.train_loss[self.epoch-1], 5)))\n","        print('for testing  data, the final classification accuracy = '+str(round(self.test_precision[self.epoch-1], 5))+\n","              ', and loss = '+str(round(self.test_loss[self.epoch-1], 5)))\n","    \n","    # process logistic regression\n","    def Training(self, epoch, optimizer, minibatch=32):\n","        self.epoch = epoch\n","        self.optimizer = optimizer\n","        self.minibatch = minibatch\n","        \n","        for i in range(epoch):\n","            # model prediction\n","            self.Prediction()\n","            \n","            # normalize probability\n","            self.Softmax()\n","            \n","            # estimation prediction performance\n","            self.Cross_entropy()\n","            \n","            # calculate precision\n","            self.Precision()\n","\n","            # back-propagation with different method\n","            if self.optimizer == 'Batch GD':\n","                self.Batch_GD()\n","            elif self.optimizer == 'SGD':\n","                self.SGD()\n","            elif self.optimizer == 'Minibatch SGD':\n","                self.Minibatch_SGD()\n","        \n","        # plot learning curve with loss and accuracy\n","        print('For ' + self.optimizer + ' :')\n","        self.Plot()\n","        \n","        # display final information\n","        self.Display()"]}]}