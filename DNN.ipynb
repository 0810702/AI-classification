{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+z4+Gf74rZ1ZAN3zRcrLf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AopKX3weBMSQ"},"outputs":[],"source":["class FCNN():\n","    def __init__(self, x, y, classes=2, minibatch=280, h1=20, h2=10):\n","        # data parameter\n","        self.x, self.y = x, y\n","        self.size = x.shape[0] # dataset size\n","        self.D = x.shape[1] # dataset features (dimension)\n","        self.classes = classes # number of classes\n","        self.minibatch = minibatch\n","        \n","        # hidden layer parameters\n","        self.h1, self.h2 = h1, h2 # number of hidden units in first and second hidden layers\n","        self.W0 = np.zeros((self.D, self.h1))\n","        self.W1 = np.zeros((self.h1, self.h2))\n","        self.Wout = np.zeros((self.h2, classes))\n","        \n","        self.b0 = np.zeros((1, h1))\n","        self.b1 = np.zeros((1, h2))\n","        self.bout = np.zeros((1, classes))\n","        \n","        # performance parameter\n","        self.eta = 0.0025\n","        self.label_class = np.argmax(self.y, axis=1)\n","        self.loss, self.precision = [], []\n","    \n","    # hidden layer operation\n","    def Hidden_layer(self, x, w, b):\n","        return np.dot(x, w) + b\n","    \n","    # activation function of hidden layer\n","    def Sigmoid(self, x):\n","        return 1 / (1 + np.exp(-x))\n","    \n","    # softmax function for output layer\n","    # z is the unnormalized probability\n","    def Softmax(self, z):\n","        exp_z = np.exp(z)\n","        exp_sum = np.sum(exp_z, axis=1).reshape(np.shape(exp_z)[0], 1)\n","        return exp_z / exp_sum\n","    \n","    # calculate cross-entropy loss and save it during training\n","    def Cross_entropy(self):  # y have pass through softmax function\n","        E = 0\n","        q = np.log2(self.y_pred.T)\n","        for i in range(self.size):\n","            E -= self.y[i][0] * q[0][i] + self.y[i][1] * q[1][i]\n","        self.loss.append(E)\n","    \n","    # calculate the classification accuracy\n","    def Precision(self):\n","        # convert one-hot to classes\n","        y_pred_class = np.argmax(self.y_pred, axis=1)\n","        count = 0\n","        for i in range(self.size):\n","            if y_pred_class[i] == self.label_class[i]:\n","                count += 1\n","        self.precision.append(count / self.size)\n","    \n","    # gradient of (softmax + cross-entropy) loss\n","    def Softmax_CrossEntropy_Derivative(self, y_pred, y):\n","        return y_pred - y\n","    \n","    # activation function derivative\n","    def Sigmoid_Derivative(self, x):\n","        return self.Sigmoid(x) * (1 - self.Sigmoid(x))\n","    \n","    # weight derivative\n","    def Weight_Gradient(self, a, y):\n","        return np.dot(a.T, y)\n","    \n","    # bias derivative\n","    def Bias_Gradient(self, y):\n","        return np.sum(y, axis=0)\n","    \n","    # forward-propagation\n","    def Forward(self):\n","        # first hidden layer\n","        self.a0 = self.Hidden_layer(self.x, self.W0, self.b0)\n","        self.y0 = self.Sigmoid(self.a0)\n","        \n","        # second hidden layer\n","        self.a1 = self.Hidden_layer(self.y0, self.W1, self.b1)\n","        self.y1 = self.Sigmoid(self.a1)\n","        \n","        # output layer\n","        self.aout = self.Hidden_layer(self.y1, self.Wout, self.bout)\n","        self.y_pred = self.Softmax(self.aout)\n","        \n","        self.Cross_entropy()\n","        self.Precision()\n","    \n","    def Derivative_sigmoid(self, x):\n","        return self.Sigmoid(x) * (1-self.Sigmoid(x))\n","    \n","    # backward-propagation\n","    def Backward(self):\n","        # create minibatch data\n","        sgd_x = np.zeros((self.minibatch, self.D))\n","        sgd_a0 = np.zeros((self.minibatch, self.h1))\n","        sgd_y0 = np.zeros((self.minibatch, self.h1))\n","        sgd_a1 = np.zeros((self.minibatch, self.h2))\n","        sgd_y1 = np.zeros((self.minibatch, self.h2))\n","        sgd_y_pred = np.zeros((self.minibatch, self.classes))\n","        sgd_y = np.zeros((self.minibatch, self.classes))\n","        \n","        # randomly choose minibatch data to update gradient\n","        rand_index = np.random.choice(self.size, self.minibatch, replace=False)\n","        \n","        for i in range(len(rand_index)):\n","            sgd_x[i, :] = self.x[rand_index[i], :]\n","            sgd_a0[i, :] = self.a0[rand_index[i], :]\n","            sgd_y0[i, :] = self.y0[rand_index[i], :]\n","            sgd_a1[i, :] = self.a1[rand_index[i], :]\n","            sgd_y1[i, :] = self.y1[rand_index[i], :]\n","            sgd_y_pred[i, :] = self.y_pred[rand_index[i], :]\n","            sgd_y[i, :] = self.y[rand_index[i], :]\n","\n","        # calculate gradient for each parameter\n","        grad_y_pred = sgd_y_pred - sgd_y  # minibatch * 2\n","        grad_wout = np.dot(sgd_y1.T, grad_y_pred)  # H2 * 2\n","        grad_bout = np.sum(grad_y_pred, axis=0)  # 1 * 2\n","\n","        grad_y1 = np.dot(grad_y_pred, self.Wout.T) * self.Derivative_sigmoid(sgd_a1) # minibatch * H2\n","        grad_w1 = np.dot(sgd_y0.T, grad_y1)  # H1 * H2\n","        grad_b1 = np.sum(grad_y1, axis=0)  # 1* H2\n","\n","        grad_y0 = np.dot(grad_y1, self.W1.T) * self.Derivative_sigmoid(sgd_a0) # minibatch * H\n","        grad_w0 = np.dot(sgd_x.T, grad_y0)  # D * H1\n","        grad_b0 = np.sum(grad_y0, axis=0)  # 1* H1\n","    \n","        # update weights and bias\n","        self.W0 -= grad_w0 * self.eta\n","        self.b0 -= grad_b0 * self.eta\n","        \n","        self.W1 -= grad_w1 * self.eta\n","        self.b1 -= grad_b1 * self.eta\n","        \n","        self.Wout -= grad_wout * self.eta\n","        self.bout -= grad_bout * self.eta\n","    \n","    # plot learning curve\n","    def Plot(self):\n","        x_axis = list(range(self.epoch))\n","        \n","        # plot learning curve\n","        plt.subplot(1, 2, 1)\n","        plt.plot(x_axis, self.loss, color='blue', label='training loss')\n","        plt.legend()\n","        \n","        # plot precision curve\n","        plt.subplot(1, 2, 2)\n","        percetage = [round(i, 2)*100 for i in self.precision]\n","        plt.plot(x_axis, percetage, color='blue', label='training accuracy')\n","        plt.legend()\n","\n","        plt.show()\n","    \n","    # output final loss and accuracy\n","    def Final_Result(self):\n","        print('Final training   loss   = ' + str(self.loss[self.epoch-1]))\n","        percentage = round(self.precision[self.epoch-1], 4) * 100\n","        print('Final training accuracy = ' + str(percentage) + ' %')\n","    \n","    # training main function\n","    def Training(self, epoch):\n","        self.epoch = epoch\n","        self.count = 0\n","        # training\n","        for i in range(self.epoch):\n","            self.count += 1\n","            self.Forward()\n","            self.Backward()\n","        \n","        # plot loss and accuracy\n","        self.Plot()\n","        self.Final_Result()"]}]}